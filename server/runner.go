package main

import (
	"context"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"runtime"
	"syscall"

	"github.com/coreos/etcd/clientv3"
	"github.com/opencontainers/runc/libcontainer"
	"github.com/opencontainers/runc/libcontainer/configs"
	_ "github.com/opencontainers/runc/libcontainer/nsenter"
	"golang.org/x/sys/unix"

	"github.com/arthurfabre/scheduler/api"
)

// Allow us to use ourselves as the container init
// nicked from https://github.com/opencontainers/runc/tree/master/libcontainer#using-libcontainer
func init() {
	if len(os.Args) > 1 && os.Args[1] == "init" {
		runtime.GOMAXPROCS(1)
		runtime.LockOSThread()
		factory, _ := libcontainer.New("")
		if err := factory.StartInitialization(); err != nil {
			log.Fatalln(err)
		}
		log.Fatalln("libcontainer init error, factory.StartInitialization() returned")
	}
}

// config creates a libcontainer config for a given rootDs and cgroupName
func config(rootFs string, cgroupName string) *configs.Config {
	defaultMountFlags := syscall.MS_NOEXEC | syscall.MS_NOSUID | syscall.MS_NODEV

	// nicked from https://github.com/opencontainers/runc/tree/master/libcontainer#using-libcontainer
	// adapted with config.josn generated by `runc spec`
	return &configs.Config{
		Rootfs: rootFs,
		Capabilities: &configs.Capabilities{
			Bounding: []string{
				"CAP_AUDIT_WRITE",
				"CAP_KILL",
				"CAP_NET_BIND_SERVICE",
			},
			Effective: []string{
				"CAP_AUDIT_WRITE",
				"CAP_KILL",
				"CAP_NET_BIND_SERVICE",
			},
			Inheritable: []string{
				"CAP_AUDIT_WRITE",
				"CAP_KILL",
				"CAP_NET_BIND_SERVICE",
			},
			Permitted: []string{
				"CAP_AUDIT_WRITE",
				"CAP_KILL",
				"CAP_NET_BIND_SERVICE",
			},
			Ambient: []string{
				"CAP_AUDIT_WRITE",
				"CAP_KILL",
				"CAP_NET_BIND_SERVICE",
			},
		},
		Namespaces: configs.Namespaces([]configs.Namespace{
			{Type: configs.NEWNS},
			{Type: configs.NEWUTS},
			{Type: configs.NEWIPC},
			{Type: configs.NEWPID},
			{Type: configs.NEWUSER},
			{Type: configs.NEWNET},
		}),
		Cgroups: &configs.Cgroup{
			Name:   cgroupName,
			Parent: "system",
			Resources: &configs.Resources{
				MemorySwappiness: nil,
				AllowAllDevices:  nil,
				AllowedDevices:   configs.DefaultAllowedDevices,
			},
		},
		MaskPaths: []string{
			"/proc/kcore",
			"/sys/firmware",
		},
		ReadonlyPaths: []string{
			"/proc/sys", "/proc/sysrq-trigger", "/proc/irq", "/proc/bus",
		},
		Devices:  configs.DefaultAutoCreatedDevices,
		Hostname: "testing",
		Mounts: []*configs.Mount{
			{
				Source:      "proc",
				Destination: "/proc",
				Device:      "proc",
				Flags:       defaultMountFlags,
			},
			{
				Source:      "tmpfs",
				Destination: "/dev",
				Device:      "tmpfs",
				Flags:       unix.MS_NOSUID | unix.MS_STRICTATIME,
				Data:        "mode=755",
			},
			{
				Source:      "devpts",
				Destination: "/dev/pts",
				Device:      "devpts",
				Flags:       unix.MS_NOSUID | unix.MS_NOEXEC,
				Data:        "newinstance,ptmxmode=0666,mode=0620,gid=5",
			},
			{
				Device:      "tmpfs",
				Source:      "shm",
				Destination: "/dev/shm",
				Data:        "mode=1777,size=65536k",
				Flags:       defaultMountFlags,
			},
			{
				Source:      "mqueue",
				Destination: "/dev/mqueue",
				Device:      "mqueue",
				Flags:       defaultMountFlags,
			},
			{
				Source:      "sysfs",
				Destination: "/sys",
				Device:      "sysfs",
				Flags:       defaultMountFlags | unix.MS_RDONLY,
			},
		},
		UidMappings: []configs.IDMap{
			{
				ContainerID: 0,
				HostID:      1000,
				Size:        65536,
			},
		},
		GidMappings: []configs.IDMap{
			{
				ContainerID: 0,
				HostID:      1000,
				Size:        65536,
			},
		},
		Networks: []*configs.Network{
			{
				Type:    "loopback",
				Address: "127.0.0.1/0",
				Gateway: "localhost",
			},
		},
		Rlimits: []configs.Rlimit{
			{
				Type: unix.RLIMIT_NOFILE,
				Hard: uint64(1025),
				Soft: uint64(1025),
			},
		},
	}
}

// process creates a libcontainer Process from a Task
func process(task *Task) (*libcontainer.Process, error) {
	log, err := os.Create(getLog(task.Id))
	if err != nil {
		return nil, err
	}

	return &libcontainer.Process{
		Args:   append([]string{task.Request.Command}, task.Request.Args...),
		Env:    []string{"PATH=/bin"},
		User:   "root",
		Stdin:  nil,
		Stdout: log,
		Stderr: log,
	}, nil
}

type Runner struct {
	client *clientv3.Client
	id     *api.NodeID
}

// watchCancel watches a Task for cancellation, killing process when it is.
// True is written to returned channel IFF the task is cancelled
func (r *Runner) watchCancel(task *Task, process *libcontainer.Process, ctx context.Context) <-chan bool {
	cancel := make(chan bool)

	// Watch for the task to be canceled.
	go func() {
		defer close(cancel)

		for taskEvent := range task.watch(ctx, r.client) {
			switch taskEvent.(type) {
			case TaskUpdate:
				taskUpdate := taskEvent.(TaskUpdate)
				switch taskUpdate.task.Status.Status.(type) {
				case *api.TaskStatus_Canceled_:
					// Only expected status change
				default:
					log.Println("WARN: Unepexcted modifiction of Task while running:", taskUpdate)
				}
			case TaskDelete:
				log.Println("WARN: Unexpected Task deletion while running")
			case TaskError:
				log.Println("WARN: Error watching Task for cancelation:", taskEvent.(TaskError).err)
			}

			process.Signal(os.Kill)
			cancel <- true
		}
	}()

	return cancel
}

// run executes a Task in a container. Error indicates task was not able to be run.
func (r *Runner) run(ctx context.Context, task *Task, factory libcontainer.Factory, cfg *configs.Config) error {
	container, err := factory.Create(task.Id.Uuid, cfg)
	if err != nil {
		return fmt.Errorf("Error creating container: %s", err)
	}
	defer container.Destroy()

	taskProcess, err := process(task)
	if err != nil {
		return fmt.Errorf("Error creating task process: %s", err)
	}

	// cancelCancel cancels the context used for task cancelation watching
	cancelCtx, cancelCancel := context.WithCancel(ctx)
	cancel := r.watchCancel(task, taskProcess, cancelCtx)
	defer cancelCancel()

	err = container.Run(taskProcess)
	if err != nil {
		return fmt.Errorf("Error running task process: %s", err)
	}

	taskState, waitErr := taskProcess.Wait()
	cancelCancel()

	select {
	// Task was cancelled, ignore waitErr as it's caused by kill()
	case <-cancel:
		return nil

	// Task finished normally
	default:
		if waitErr != nil {
			return fmt.Errorf("Error waiting for task process: %s", waitErr)
		}

		taskStatus, ok := taskState.Sys().(syscall.WaitStatus)
		if !ok {
			return fmt.Errorf("Error getting task process exit code")
		}

		err = task.complete(context.Background(), r.client, r.id, taskStatus.ExitStatus())
		if err != nil {
			return fmt.Errorf("Error completing task: %s", err)
		}

		return nil
	}
}

func (r *Runner) Start(ctx context.Context, containerDir string, rootFs string) {
	rootFs, err := filepath.Abs(rootFs)
	if err != nil {
		log.Fatalln("Error getting absolute rootfs path", err)
	}

	factory, err := libcontainer.New(containerDir, libcontainer.Cgroupfs, libcontainer.InitArgs(os.Args[0], "init"))
	if err != nil {
		log.Fatalln("Error creating libcontainer factory:", err)
	}

	// TODO - What does cgroupName do?
	cfg := config(rootFs, "test")

	newTasks := watchQueuedTasks(ctx, r.client)

	for taskEvent := range newTasks {
		switch taskEvent.(type) {
		case TaskUpdate:
			task := taskEvent.(TaskUpdate).task

			if err := task.run(ctx, r.client, r.id); err != nil {
				// TODO - Differentiate stolen task from other errors
				continue
			}

			log.Println("Running task", task.Id.Uuid)

			go func(task *Task) {
				err := r.run(ctx, task, factory, cfg)
				if err == nil {
					return
				}

				log.Println("Error running task, re-queuing:", err)
				err = task.queue(ctx, r.client)
				if err != nil {
					// Not much we can do at this point...
					log.Println("Error re-queuing failed task:", err)
				}
			}(task)

		case TaskError:
			log.Println("Error watching for queued tasks:", taskEvent.(TaskError).err)
		}
	}
}
